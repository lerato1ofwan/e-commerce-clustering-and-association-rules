{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1450a0a2-e58d-4615-9c14-3ba0cf17f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup and Data Loading\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture # Python Implementation of MClust\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c70b1719-dbb1-4253-a883-49c603d0ad5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = '../dataset'  # Go up one level, then into 'dataset'\n",
    "\n",
    "df_events = pd.read_csv(f'{BASE_DIR}/events.csv')\n",
    "items_part1 = pd.read_csv(f'{BASE_DIR}/item_properties_part1.csv')\n",
    "items_part2 = pd.read_csv(f'{BASE_DIR}/item_properties_part2.csv')\n",
    "\n",
    "df_items = pd.concat([items_part1, items_part2], ignore_index=True)\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1455429-45c0-4c2a-874a-e1fab5d319a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1352 bot-like visitors based on 99.9th percentile event count.\n",
      "Remaining events: 2536174\n"
     ]
    }
   ],
   "source": [
    "#Data Cleaning and Outlier Removal\n",
    "\n",
    "# 2.1 Convert timestamp and handle missing transactions\n",
    "df_events['timestamp'] = pd.to_datetime(df_events['timestamp'], unit='ms')\n",
    "# Fill missing transaction IDs with 0 to preserve non-transactional events (views/carts)\n",
    "df_events['transactionid'] = df_events['transactionid'].fillna(0) \n",
    "\n",
    "# 2.2 Remove high-activity outliers (Bots/Scrapers)\n",
    "# Find event counts per visitor\n",
    "visitor_counts = df_events['visitorid'].value_counts()\n",
    "\n",
    "# Identify the 99.9th percentile threshold for event counts\n",
    "threshold = visitor_counts.quantile(0.999) \n",
    "bot_visitor_ids = visitor_counts[visitor_counts > threshold].index\n",
    "\n",
    "# Filter the events dataframe\n",
    "df_events = df_events[~df_events['visitorid'].isin(bot_visitor_ids)]\n",
    "print(f\"Removed {len(bot_visitor_ids)} bot-like visitors based on 99.9th percentile event count.\")\n",
    "print(f\"Remaining events: {len(df_events)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a44bc0-2163-4674-b0d0-b84fd4f5f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique sessions created: 1722864\n"
     ]
    }
   ],
   "source": [
    "#Advanced Sessionization\n",
    "\n",
    "# 3.1 Session Creation using a 30-minute (1800 second) inactivity timeout\n",
    "df_events = df_events.sort_values(['visitorid', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "# Calculate time difference between consecutive events for the same visitor\n",
    "df_events['time_diff'] = df_events.groupby('visitorid')['timestamp'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "# Mark the start of a new session if time difference > 1800 seconds\n",
    "df_events['new_session'] = (df_events['time_diff'] > 1800).astype(int)\n",
    "\n",
    "# Create a session_id unique within each visitor\n",
    "df_events['session_id'] = df_events.groupby('visitorid')['new_session'].cumsum()\n",
    "\n",
    "# Create a globally unique session identifier\n",
    "df_events['global_session_id'] = df_events['visitorid'].astype(str) + '_' + df_events['session_id'].astype(str)\n",
    "\n",
    "print(f\"Total unique sessions created: {df_events['global_session_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de07e4ec-81ac-4afd-885a-60845d0e3911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal merge with item properties completed.\n"
     ]
    }
   ],
   "source": [
    "# Temporal Merging (If Item Properties are Time-Dependent)\n",
    "\n",
    "# NOTE: This step is only necessary if df_items contains time-sensitive properties \n",
    "# (e.g., price changes, stock status) that were used in the original notebook.\n",
    "# We include it here for completeness as an example of good feature engineering.\n",
    "\n",
    "# 1. Prepare item properties: convert timestamp and sort\n",
    "df_items['timestamp'] = pd.to_datetime(df_items['timestamp'], unit='ms')\n",
    "df_items = df_items.sort_values('timestamp')\n",
    "\n",
    "# 2. Merge: Join each event with the most recent item property status *before* that event.\n",
    "# This requires both dataframes to be sorted on the 'on' column ('timestamp').\n",
    "df_events = pd.merge_asof(\n",
    "    df_events.sort_values('timestamp'),\n",
    "    df_items[['itemid', 'timestamp', 'property', 'value']],\n",
    "    on='timestamp', \n",
    "    by='itemid', \n",
    "    direction='backward',\n",
    "    suffixes=('', '_itemprop')\n",
    ")\n",
    "\n",
    "print(\"Temporal merge with item properties completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887e3d5-4260-4668-a85b-314be85978a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Behavioral and Ratio Features\n",
    "\n",
    "# 1. Aggregate event data to the session level\n",
    "agg_features = df_events.groupby('global_session_id').agg(\n",
    "    # Core Counts\n",
    "    total_events=('event', 'count'),\n",
    "    view_count=('event', lambda x: (x == 'view').sum()),\n",
    "    addtocart_count=('event', lambda x: (x == 'addtocart').sum()),\n",
    "    transaction_count=('event', lambda x: (x == 'transaction').sum()),\n",
    "    # Unique Items Viewed (for variety measure)\n",
    "    unique_items_viewed=('itemid', lambda x: x[x.index.isin(df_events[df_events['event'] == 'view'].index)].nunique()),\n",
    "    # Temporal features\n",
    "    session_duration_sec=('timestamp', lambda x: (x.max() - x.min()).total_seconds()),\n",
    "    session_hour_of_day=('timestamp', lambda x: x.iloc[0].hour),\n",
    "    # Target feature (for analysis later)\n",
    "    is_buyer=('transactionid', lambda x: 1 if (x > 0).any() else 0)\n",
    ").reset_index()\n",
    "\n",
    "# 2. Calculate Key Behavioral Ratio Features\n",
    "\n",
    "# View-to-Cart Ratio: Efficiency of viewing leading to cart addition\n",
    "agg_features['view_to_cart_ratio'] = (\n",
    "    agg_features['addtocart_count'] / agg_features['view_count'].replace(0, np.nan)\n",
    ").fillna(0)\n",
    "\n",
    "# Event Rate per Second: Intensity of user engagement\n",
    "# Use a small constant (1e-6) instead of 0 for duration to prevent division by zero for 1-event sessions\n",
    "agg_features['event_rate_per_sec'] = (\n",
    "    agg_features['total_events'] / agg_features['session_duration_sec'].replace(0, 1e-6)\n",
    ")\n",
    "\n",
    "print(\"Aggregated and ratio features created.\")\n",
    "print(agg_features[['total_events', 'view_to_cart_ratio', 'event_rate_per_sec']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45accc-54f7-4eb5-966b-ba298e16149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MClust/GMM Clustering Implementation\n",
    "\n",
    "# 1. Determine the optimal number of clusters (K)\n",
    "# Use Bayesian Information Criterion (BIC) to select the best K and covariance type\n",
    "N_COMPONENTS = range(2, 11)\n",
    "bic_scores = []\n",
    "cov_types = ['full', 'tied', 'diag', 'spherical']\n",
    "\n",
    "for n_comp in N_COMPONENTS:\n",
    "    for cov in cov_types:\n",
    "        try:\n",
    "            gmm = GaussianMixture(n_components=n_comp, covariance_type=cov, random_state=42)\n",
    "            gmm.fit(X_scaled)\n",
    "            bic_scores.append((gmm.bic(X_scaled), n_comp, cov))\n",
    "        except ValueError:\n",
    "            # Catch cases where GMM fails to converge for certain settings\n",
    "            continue\n",
    "\n",
    "# Find the configuration with the minimum BIC score\n",
    "best_bic, best_k, best_cov = min(bic_scores, key=lambda x: x[0])\n",
    "print(f\"Optimal GMM Configuration: K={best_k}, Covariance Type='{best_cov}', BIC={best_bic:.2f}\")\n",
    "\n",
    "\n",
    "# 2. Run the final GMM model\n",
    "final_gmm = GaussianMixture(n_components=best_k, covariance_type=best_cov, random_state=42)\n",
    "final_gmm.fit(X_scaled)\n",
    "agg_features['gmm_cluster'] = final_gmm.predict(X_scaled)\n",
    "\n",
    "print(\"\\nGMM Clustering complete. Clusters assigned to session features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d5a6a-499f-4d95-a5f7-2d154812633f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
