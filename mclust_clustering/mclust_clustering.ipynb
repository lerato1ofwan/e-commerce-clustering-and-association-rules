{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1450a0a2-e58d-4615-9c14-3ba0cf17f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "events = pd.read_csv('dataset/events.csv')\n",
    "# Load both parts\n",
    "items_part1 = pd.read_csv('dataset/item_properties_part1.csv')\n",
    "items_part2 = pd.read_csv('dataset/item_properties_part2.csv')\n",
    "\n",
    "# Concatenate them vertically (assuming same columns)\n",
    "items = pd.concat([items_part1, items_part2], ignore_index=True)\n",
    "\n",
    "# Create sessions (flows)\n",
    "# Group events by visitor within time windows (e.g., 30 min inactivity = new session)\n",
    "events['timestamp'] = pd.to_datetime(events['timestamp'], unit='ms')\n",
    "events = events.sort_values(['visitorid', 'timestamp'])\n",
    "\n",
    "def create_sessions(df, timeout_minutes=30):\n",
    "    df['time_diff'] = df.groupby('visitorid')['timestamp'].diff()\n",
    "    df['new_session'] = (df['time_diff'] > pd.Timedelta(minutes=timeout_minutes)) | df['time_diff'].isna()\n",
    "    df['sessionid'] = df.groupby('visitorid')['new_session'].cumsum()\n",
    "    return df\n",
    "\n",
    "events = create_sessions(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d636fdf4-dcef-4898-82a1-98f2c9f8122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "def extract_flow_features(events_df):\n",
    "    \"\"\"\n",
    "    Extract features for each session (flow)\n",
    "    Similar to flow statistics in network traffic\n",
    "    \"\"\"\n",
    "    flow_features = events_df.groupby(['visitorid', 'sessionid']).agg({\n",
    "        'timestamp': ['min', 'max', 'count'],  # Session start, end, event count\n",
    "        'itemid': 'nunique',  # Unique items viewed\n",
    "        'event': lambda x: (x == 'view').sum(),  # View count\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    flow_features.columns = ['visitorid', 'sessionid', 'start_time', 'end_time', \n",
    "                              'event_count', 'unique_items', 'view_count']\n",
    "    \n",
    "    # Calculate derived features\n",
    "    flow_features['duration_seconds'] = (\n",
    "        flow_features['end_time'] - flow_features['start_time']\n",
    "    ).dt.total_seconds()\n",
    "    \n",
    "    # Add behavioral features\n",
    "    addtocart_counts = events_df[events_df['event'] == 'addtocart'].groupby(\n",
    "        ['visitorid', 'sessionid']\n",
    "    ).size().reset_index(name='addtocart_count')\n",
    "    \n",
    "    transaction_counts = events_df[events_df['event'] == 'transaction'].groupby(\n",
    "        ['visitorid', 'sessionid']\n",
    "    ).size().reset_index(name='transaction_count')\n",
    "    \n",
    "    flow_features = flow_features.merge(addtocart_counts, on=['visitorid', 'sessionid'], how='left')\n",
    "    flow_features = flow_features.merge(transaction_counts, on=['visitorid', 'sessionid'], how='left')\n",
    "    flow_features = flow_features.fillna(0)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    flow_features['view_to_cart_ratio'] = flow_features['addtocart_count'] / (flow_features['view_count'] + 1)\n",
    "    flow_features['cart_to_purchase_ratio'] = flow_features['transaction_count'] / (flow_features['addtocart_count'] + 1)\n",
    "    flow_features['events_per_minute'] = flow_features['event_count'] / (flow_features['duration_seconds'] / 60 + 1)\n",
    "    \n",
    "    # Time-based features\n",
    "    flow_features['hour_of_day'] = flow_features['start_time'].dt.hour\n",
    "    flow_features['day_of_week'] = flow_features['start_time'].dt.dayofweek\n",
    "    \n",
    "    return flow_features\n",
    "\n",
    "flow_data = extract_flow_features(events)\n",
    "flow_data.to_csv(\"flow_features.csv\", index=False)  # Add this for output like the first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26a3c5a-c8d5-4061-85d8-5b2a1c0314d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 4: ASSOCIATION RULE MINING\n",
      "================================================================================\n",
      "\n",
      "Loaded clustered flow data: (1761675, 16)\n",
      "Clusters found: 1\n",
      "\n",
      "Cluster distribution:\n",
      "cluster\n",
      "1    1761675\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Loaded events data: (2756101, 5)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4.1: PRODUCT-LEVEL ASSOCIATION RULES (MARKET BASKET ANALYSIS)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total transactions: 17672\n",
      "Transactions with 2+ items: 2710\n",
      "\n",
      "Mining frequent itemsets...\n",
      "Frequent itemsets found: 10\n",
      "Generating association rules...\n",
      "\n",
      "Product Association Rules Generated: 2\n",
      "\n",
      "Top 10 Product Rules by Lift:\n",
      "  antecedents consequents   support  confidence       lift\n",
      "0    (213834)    (445351)  0.014391    0.423913  29.456522\n",
      "1    (445351)    (213834)  0.014391    1.000000  29.456522\n",
      "\n",
      "✓ Saved to 'product_association_rules.csv'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4.2: CATEGORY-LEVEL ASSOCIATION RULES\n",
      "--------------------------------------------------------------------------------\n",
      "item_properties.csv not found - skipping category-level rules\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4.3: BEHAVIORAL PATTERN ASSOCIATION RULES (CLUSTER-BASED)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Behavioral feature matrix shape: (1761675, 6)\n",
      "Features: ['cluster_1', 'high_engagement', 'cart_user', 'is_converter', 'long_session', 'multi_item_viewer']\n",
      "\n",
      "Mining behavioral patterns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kamoh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:161: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent behavioral patterns: 15\n",
      "\n",
      "Behavioral Association Rules: 36\n",
      "Rules predicting conversion/cart: 0\n",
      "\n",
      "Top 15 Behavioral Rules (Predicting Conversion/Cart):\n",
      "Empty DataFrame\n",
      "Columns: [antecedents, consequents, support, confidence, lift]\n",
      "Index: []\n",
      "\n",
      "✓ Saved to 'behavioral_association_rules.csv'\n",
      "✓ Saved to 'conversion_prediction_rules.csv'\n",
      "\n",
      "================================================================================\n",
      "PHASE 5: REFINEMENT - RULE-BASED FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5.1: CREATING RULE-BASED FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "No behavioral rules available for feature creation\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5.2: RE-CLUSTERING WITH ENRICHED FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Original features: 9\n",
      "Rule-based features: 0\n",
      "Total enriched features: 9\n",
      "\n",
      "Finding optimal number of clusters with BIC...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 312\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n_components \u001b[38;5;129;01min\u001b[39;00m n_components_range:\n\u001b[32m    307\u001b[39m     gmm = GaussianMixture(n_components=n_components, \n\u001b[32m    308\u001b[39m                          covariance_type=\u001b[33m'\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    309\u001b[39m                          random_state=\u001b[32m42\u001b[39m, \n\u001b[32m    310\u001b[39m                          n_init=\u001b[32m10\u001b[39m,\n\u001b[32m    311\u001b[39m                          max_iter=\u001b[32m200\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[43mgmm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_enriched_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     bic_scores.append(gmm.bic(X_enriched_scaled))\n\u001b[32m    314\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m clusters: BIC = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbic_scores[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\mixture\\_base.py:182\u001b[39m, in \u001b[36mBaseMixture.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[32m    157\u001b[39m \n\u001b[32m    158\u001b[39m \u001b[33;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    179\u001b[39m \u001b[33;03m    The fitted mixture.\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\mixture\\_base.py:250\u001b[39m, in \u001b[36mBaseMixture.fit_predict\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.max_iter + \u001b[32m1\u001b[39m):\n\u001b[32m    248\u001b[39m     prev_lower_bound = lower_bound\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     log_prob_norm, log_resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28mself\u001b[39m._m_step(X, log_resp)\n\u001b[32m    252\u001b[39m     lower_bound = \u001b[38;5;28mself\u001b[39m._compute_lower_bound(log_resp, log_prob_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\mixture\\_base.py:312\u001b[39m, in \u001b[36mBaseMixture._e_step\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_e_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    297\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"E step.\u001b[39;00m\n\u001b[32m    298\u001b[39m \n\u001b[32m    299\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m        the point of each sample in X.\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     log_prob_norm, log_resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_estimate_log_prob_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.mean(log_prob_norm), log_resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\mixture\\_base.py:532\u001b[39m, in \u001b[36mBaseMixture._estimate_log_prob_resp\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_estimate_log_prob_resp\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    514\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Estimate log probabilities and responsibilities for each sample.\u001b[39;00m\n\u001b[32m    515\u001b[39m \n\u001b[32m    516\u001b[39m \u001b[33;03m    Compute the log probabilities, weighted log probabilities per\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    530\u001b[39m \u001b[33;03m        logarithm of the responsibilities\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     weighted_log_prob = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_estimate_weighted_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m     log_prob_norm = logsumexp(weighted_log_prob, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(under=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    535\u001b[39m         \u001b[38;5;66;03m# ignore underflow\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\mixture\\_base.py:485\u001b[39m, in \u001b[36mBaseMixture._estimate_weighted_log_prob\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_estimate_weighted_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    475\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Estimate the weighted log-probabilities, log P(X | Z) + log weights.\u001b[39;00m\n\u001b[32m    476\u001b[39m \n\u001b[32m    477\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    483\u001b[39m \u001b[33;03m    weighted_log_prob : array, shape (n_samples, n_component)\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_estimate_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m + \u001b[38;5;28mself\u001b[39m._estimate_log_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:839\u001b[39m, in \u001b[36mGaussianMixture._estimate_log_prob\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_estimate_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_estimate_log_gaussian_prob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmeans_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecisions_cholesky_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcovariance_type\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:487\u001b[39m, in \u001b[36m_estimate_log_gaussian_prob\u001b[39m\u001b[34m(X, means, precisions_chol, covariance_type)\u001b[39m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, (mu, prec_chol) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(means, precisions_chol)):\n\u001b[32m    486\u001b[39m         y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m         log_prob[:, k] = np.sum(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m covariance_type == \u001b[33m\"\u001b[39m\u001b[33mtied\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    490\u001b[39m     log_prob = np.empty((n_samples, n_components), dtype=X.dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 4: ASSOCIATION RULE MINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 4: ASSOCIATION RULE MINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the clustered flow data from R\n",
    "flow_features_clustered = pd.read_csv('flow_features_clustered.csv')\n",
    "print(f\"\\nLoaded clustered flow data: {flow_features_clustered.shape}\")\n",
    "print(f\"Clusters found: {flow_features_clustered['cluster'].nunique()}\")\n",
    "print(f\"\\nCluster distribution:\\n{flow_features_clustered['cluster'].value_counts().sort_index()}\")\n",
    "\n",
    "# Also load original events data for transaction-level rules\n",
    "events = pd.read_csv('events.csv')\n",
    "events['timestamp'] = pd.to_datetime(events['timestamp'], unit='ms')\n",
    "print(f\"\\nLoaded events data: {events.shape}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4.1: Product-Level Association Rules (Market Basket Analysis)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4.1: PRODUCT-LEVEL ASSOCIATION RULES (MARKET BASKET ANALYSIS)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Get transactions only\n",
    "transactions = events[events['event'] == 'transaction'].copy()\n",
    "print(f\"\\nTotal transactions: {transactions['transactionid'].nunique()}\")\n",
    "\n",
    "# Group items by transaction\n",
    "transaction_lists = transactions.groupby('transactionid')['itemid'].apply(list).values\n",
    "\n",
    "# Filter transactions with at least 2 items\n",
    "transaction_lists = [t for t in transaction_lists if len(t) >= 2]\n",
    "print(f\"Transactions with 2+ items: {len(transaction_lists)}\")\n",
    "\n",
    "if len(transaction_lists) > 0:\n",
    "    # Encode transactions\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transaction_lists).transform(transaction_lists)\n",
    "    df_transactions_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    # Apply Apriori algorithm\n",
    "    print(\"\\nMining frequent itemsets...\")\n",
    "    frequent_itemsets_products = apriori(df_transactions_encoded, \n",
    "                                         min_support=0.005,  # Adjust based on your data\n",
    "                                         use_colnames=True,\n",
    "                                         max_len=3)\n",
    "    \n",
    "    print(f\"Frequent itemsets found: {len(frequent_itemsets_products)}\")\n",
    "    \n",
    "    if len(frequent_itemsets_products) > 0:\n",
    "        # Generate association rules\n",
    "        print(\"Generating association rules...\")\n",
    "        product_rules = association_rules(frequent_itemsets_products, \n",
    "                                         metric=\"confidence\", \n",
    "                                         min_threshold=0.3)\n",
    "        \n",
    "        # Add lift and conviction metrics\n",
    "        product_rules['lift'] = product_rules['lift']\n",
    "        product_rules = product_rules.sort_values('lift', ascending=False)\n",
    "        \n",
    "        print(f\"\\nProduct Association Rules Generated: {len(product_rules)}\")\n",
    "        print(\"\\nTop 10 Product Rules by Lift:\")\n",
    "        print(product_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "        \n",
    "        # Save product rules\n",
    "        product_rules.to_csv('product_association_rules.csv', index=False)\n",
    "        print(\"\\n✓ Saved to 'product_association_rules.csv'\")\n",
    "    else:\n",
    "        print(\"No rules generated - support threshold may be too high\")\n",
    "        product_rules = pd.DataFrame()\n",
    "else:\n",
    "    print(\"Not enough multi-item transactions for product association rules\")\n",
    "    product_rules = pd.DataFrame()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4.2: Category-Level Association Rules (if item properties available)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4.2: CATEGORY-LEVEL ASSOCIATION RULES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "try:\n",
    "    item_properties = pd.read_csv('item_properties.csv')\n",
    "    \n",
    "    # Get category information for items\n",
    "    categories = item_properties[item_properties['property'] == 'categoryid'].copy()\n",
    "    categories = categories[['itemid', 'value']].rename(columns={'value': 'categoryid'})\n",
    "    categories = categories.drop_duplicates('itemid')\n",
    "    \n",
    "    # Merge with transactions\n",
    "    transactions_with_cat = transactions.merge(categories, on='itemid', how='left')\n",
    "    \n",
    "    # Group categories by transaction\n",
    "    category_transactions = transactions_with_cat.groupby('transactionid')['categoryid'].apply(\n",
    "        lambda x: list(x.dropna().unique())\n",
    "    ).values\n",
    "    \n",
    "    category_transactions = [t for t in category_transactions if len(t) >= 2]\n",
    "    print(f\"\\nTransactions with 2+ categories: {len(category_transactions)}\")\n",
    "    \n",
    "    if len(category_transactions) > 0:\n",
    "        # Encode category transactions\n",
    "        te_cat = TransactionEncoder()\n",
    "        te_cat_ary = te_cat.fit(category_transactions).transform(category_transactions)\n",
    "        df_cat_encoded = pd.DataFrame(te_cat_ary, columns=te_cat.columns_)\n",
    "        \n",
    "        # Apply Apriori\n",
    "        frequent_itemsets_categories = apriori(df_cat_encoded, \n",
    "                                               min_support=0.01, \n",
    "                                               use_colnames=True,\n",
    "                                               max_len=3)\n",
    "        \n",
    "        print(f\"Frequent category itemsets: {len(frequent_itemsets_categories)}\")\n",
    "        \n",
    "        if len(frequent_itemsets_categories) > 0:\n",
    "            category_rules = association_rules(frequent_itemsets_categories, \n",
    "                                              metric=\"confidence\", \n",
    "                                              min_threshold=0.3)\n",
    "            category_rules = category_rules.sort_values('lift', ascending=False)\n",
    "            \n",
    "            print(f\"\\nCategory Association Rules: {len(category_rules)}\")\n",
    "            print(\"\\nTop 10 Category Rules by Lift:\")\n",
    "            print(category_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(10))\n",
    "            \n",
    "            category_rules.to_csv('category_association_rules.csv', index=False)\n",
    "            print(\"\\n✓ Saved to 'category_association_rules.csv'\")\n",
    "        else:\n",
    "            category_rules = pd.DataFrame()\n",
    "    else:\n",
    "        category_rules = pd.DataFrame()\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"item_properties.csv not found - skipping category-level rules\")\n",
    "    category_rules = pd.DataFrame()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4.3: Behavioral Pattern Association Rules (Cluster-based)\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"4.3: BEHAVIORAL PATTERN ASSOCIATION RULES (CLUSTER-BASED)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Create behavioral features\n",
    "flow_features_clustered['is_converter'] = (flow_features_clustered['transaction_count'] > 0).astype(int)\n",
    "flow_features_clustered['high_engagement'] = (\n",
    "    flow_features_clustered['event_count'] > flow_features_clustered['event_count'].median()\n",
    ").astype(int)\n",
    "flow_features_clustered['cart_user'] = (flow_features_clustered['addtocart_count'] > 0).astype(int)\n",
    "flow_features_clustered['long_session'] = (\n",
    "    flow_features_clustered['duration_seconds'] > flow_features_clustered['duration_seconds'].median()\n",
    ").astype(int)\n",
    "flow_features_clustered['multi_item_viewer'] = (\n",
    "    flow_features_clustered['unique_items'] > flow_features_clustered['unique_items'].median()\n",
    ").astype(int)\n",
    "\n",
    "# Create cluster dummy variables\n",
    "cluster_dummies = pd.get_dummies(flow_features_clustered['cluster'], prefix='cluster')\n",
    "\n",
    "# Combine all behavioral features\n",
    "behavior_features = pd.concat([\n",
    "    cluster_dummies,\n",
    "    flow_features_clustered[['high_engagement', 'cart_user', 'is_converter', \n",
    "                             'long_session', 'multi_item_viewer']]\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\nBehavioral feature matrix shape: {behavior_features.shape}\")\n",
    "print(f\"Features: {list(behavior_features.columns)}\")\n",
    "\n",
    "# Mine behavioral patterns\n",
    "print(\"\\nMining behavioral patterns...\")\n",
    "behavior_itemsets = apriori(behavior_features, \n",
    "                           min_support=0.05,  # At least 5% of sessions\n",
    "                           use_colnames=True,\n",
    "                           max_len=4)\n",
    "\n",
    "print(f\"Frequent behavioral patterns: {len(behavior_itemsets)}\")\n",
    "\n",
    "if len(behavior_itemsets) > 0:\n",
    "    # Generate behavioral rules\n",
    "    behavior_rules = association_rules(behavior_itemsets, \n",
    "                                      metric=\"lift\", \n",
    "                                      min_threshold=1.2)\n",
    "    \n",
    "    # Filter for interesting rules (rules that predict conversion or cart usage)\n",
    "    interesting_consequents = ['is_converter', 'cart_user']\n",
    "    interesting_rules = behavior_rules[\n",
    "        behavior_rules['consequents'].apply(\n",
    "            lambda x: any(item in interesting_consequents for item in x)\n",
    "        )\n",
    "    ].sort_values('lift', ascending=False)\n",
    "    \n",
    "    print(f\"\\nBehavioral Association Rules: {len(behavior_rules)}\")\n",
    "    print(f\"Rules predicting conversion/cart: {len(interesting_rules)}\")\n",
    "    \n",
    "    print(\"\\nTop 15 Behavioral Rules (Predicting Conversion/Cart):\")\n",
    "    print(interesting_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head(15))\n",
    "    \n",
    "    # Save behavioral rules\n",
    "    behavior_rules.to_csv('behavioral_association_rules.csv', index=False)\n",
    "    interesting_rules.to_csv('conversion_prediction_rules.csv', index=False)\n",
    "    print(\"\\n✓ Saved to 'behavioral_association_rules.csv'\")\n",
    "    print(\"✓ Saved to 'conversion_prediction_rules.csv'\")\n",
    "else:\n",
    "    behavior_rules = pd.DataFrame()\n",
    "    interesting_rules = pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 5: REFINEMENT - RULE-BASED FEATURE ENGINEERING & RE-CLUSTERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 5: REFINEMENT - RULE-BASED FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5.1: Create Rule-Based Features\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"5.1: CREATING RULE-BASED FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def apply_rule_as_feature(df, rule, rule_id):\n",
    "    \"\"\"\n",
    "    Apply an association rule as a binary feature\n",
    "    Returns True if all antecedents are present\n",
    "    \"\"\"\n",
    "    antecedents = list(rule['antecedents'])\n",
    "    \n",
    "    # Check if all antecedent columns exist\n",
    "    missing_cols = [col for col in antecedents if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        return pd.Series([0] * len(df), index=df.index)\n",
    "    \n",
    "    # Check if all antecedents are True (1)\n",
    "    feature = df[antecedents].all(axis=1).astype(int)\n",
    "    return feature\n",
    "\n",
    "# Apply top behavioral rules as features\n",
    "if len(interesting_rules) > 0:\n",
    "    top_n_rules = min(15, len(interesting_rules))  # Use top 15 rules or fewer\n",
    "    \n",
    "    print(f\"\\nApplying top {top_n_rules} behavioral rules as features...\")\n",
    "    \n",
    "    for idx, (rule_idx, rule) in enumerate(interesting_rules.head(top_n_rules).iterrows()):\n",
    "        feature_name = f'rule_pattern_{idx+1}'\n",
    "        flow_features_clustered[feature_name] = apply_rule_as_feature(\n",
    "            behavior_features, rule, idx\n",
    "        )\n",
    "        \n",
    "        # Print rule interpretation\n",
    "        antecedents_str = ', '.join([str(x) for x in rule['antecedents']])\n",
    "        consequents_str = ', '.join([str(x) for x in rule['consequents']])\n",
    "        print(f\"  {feature_name}: IF {antecedents_str} THEN {consequents_str} \"\n",
    "              f\"(conf={rule['confidence']:.2f}, lift={rule['lift']:.2f})\")\n",
    "    \n",
    "    rule_feature_cols = [col for col in flow_features_clustered.columns if 'rule_pattern_' in col]\n",
    "    print(f\"\\n✓ Created {len(rule_feature_cols)} rule-based features\")\n",
    "else:\n",
    "    rule_feature_cols = []\n",
    "    print(\"\\nNo behavioral rules available for feature creation\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5.2: Re-clustering with Enriched Features\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"5.2: RE-CLUSTERING WITH ENRICHED FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Define original clustering features\n",
    "original_feature_cols = ['duration_seconds', 'event_count', 'unique_items', \n",
    "                         'view_count', 'addtocart_count', 'transaction_count',\n",
    "                         'view_to_cart_ratio', 'cart_to_purchase_ratio', \n",
    "                         'events_per_minute']\n",
    "\n",
    "# Combine original + rule-based features\n",
    "enriched_feature_cols = original_feature_cols + rule_feature_cols\n",
    "\n",
    "print(f\"\\nOriginal features: {len(original_feature_cols)}\")\n",
    "print(f\"Rule-based features: {len(rule_feature_cols)}\")\n",
    "print(f\"Total enriched features: {len(enriched_feature_cols)}\")\n",
    "\n",
    "# Prepare data for re-clustering\n",
    "X_enriched = flow_features_clustered[enriched_feature_cols].fillna(0)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_enriched_scaled = scaler.fit_transform(X_enriched)\n",
    "\n",
    "# Determine optimal clusters using BIC (same as Phase 3 but with enriched features)\n",
    "print(\"\\nFinding optimal number of clusters with BIC...\")\n",
    "bic_scores = []\n",
    "n_components_range = range(2, 11)\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n_components, \n",
    "                         covariance_type='full', \n",
    "                         random_state=42, \n",
    "                         n_init=10,\n",
    "                         max_iter=200)\n",
    "    gmm.fit(X_enriched_scaled)\n",
    "    bic_scores.append(gmm.bic(X_enriched_scaled))\n",
    "    print(f\"  {n_components} clusters: BIC = {bic_scores[-1]:.2f}\")\n",
    "\n",
    "optimal_clusters_refined = n_components_range[np.argmin(bic_scores)]\n",
    "print(f\"\\n✓ Optimal number of clusters (refined): {optimal_clusters_refined}\")\n",
    "\n",
    "# Fit final refined model\n",
    "print(\"\\nFitting refined clustering model...\")\n",
    "refined_gmm = GaussianMixture(n_components=optimal_clusters_refined, \n",
    "                              covariance_type='full', \n",
    "                              random_state=42,\n",
    "                              n_init=10,\n",
    "                              max_iter=200)\n",
    "flow_features_clustered['refined_cluster'] = refined_gmm.fit_predict(X_enriched_scaled)\n",
    "\n",
    "print(f\"\\nRefined cluster distribution:\\n{flow_features_clustered['refined_cluster'].value_counts().sort_index()}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5.3: Compare Original vs Refined Clusters\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"5.3: COMPARING ORIGINAL VS REFINED CLUSTERS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Cross-tabulation\n",
    "cluster_comparison = pd.crosstab(\n",
    "    flow_features_clustered['cluster'], \n",
    "    flow_features_clustered['refined_cluster'],\n",
    "    margins=True\n",
    ")\n",
    "print(\"\\nCluster Mapping (Original → Refined):\")\n",
    "print(cluster_comparison)\n",
    "\n",
    "# Analyze conversion rates by cluster\n",
    "original_conversion = flow_features_clustered.groupby('cluster').agg({\n",
    "    'is_converter': 'mean',\n",
    "    'cart_user': 'mean',\n",
    "    'transaction_count': 'sum'\n",
    "}).round(3)\n",
    "\n",
    "refined_conversion = flow_features_clustered.groupby('refined_cluster').agg({\n",
    "    'is_converter': 'mean',\n",
    "    'cart_user': 'mean',\n",
    "    'transaction_count': 'sum'\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nOriginal Cluster Performance:\")\n",
    "print(original_conversion)\n",
    "\n",
    "print(\"\\nRefined Cluster Performance:\")\n",
    "print(refined_conversion)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5.4: Save Final Results\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"5.4: SAVING FINAL RESULTS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Save enriched flow features with both cluster assignments\n",
    "flow_features_clustered.to_csv('flow_features_refined.csv', index=False)\n",
    "print(\"✓ Saved 'flow_features_refined.csv'\")\n",
    "\n",
    "# Create cluster profiles\n",
    "cluster_profiles = flow_features_clustered.groupby('refined_cluster')[\n",
    "    original_feature_cols + ['is_converter', 'cart_user']\n",
    "].mean().round(3)\n",
    "cluster_profiles['count'] = flow_features_clustered.groupby('refined_cluster').size()\n",
    "cluster_profiles.to_csv('refined_cluster_profiles.csv')\n",
    "print(\"✓ Saved 'refined_cluster_profiles.csv'\")\n",
    "\n",
    "# Summary statistics\n",
    "summary = {\n",
    "    'total_sessions': len(flow_features_clustered),\n",
    "    'original_clusters': flow_features_clustered['cluster'].nunique(),\n",
    "    'refined_clusters': flow_features_clustered['refined_cluster'].nunique(),\n",
    "    'product_rules': len(product_rules) if len(product_rules) > 0 else 0,\n",
    "    'behavioral_rules': len(behavior_rules) if len(behavior_rules) > 0 else 0,\n",
    "    'rule_features_created': len(rule_feature_cols),\n",
    "    'overall_conversion_rate': flow_features_clustered['is_converter'].mean(),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\n✓ Phase 4 & 5 Complete!\")\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"  - product_association_rules.csv\")\n",
    "print(\"  - behavioral_association_rules.csv\")\n",
    "print(\"  - conversion_prediction_rules.csv\")\n",
    "print(\"  - flow_features_refined.csv\")\n",
    "print(\"  - refined_cluster_profiles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d5a6a-499f-4d95-a5f7-2d154812633f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
